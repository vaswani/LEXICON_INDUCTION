#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 1.5cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Improved Bilingual Lexicon Learning using Monolingual Graphs
\end_layout

\begin_layout Abstract
In the context of language translation, it is likely that synonyms (or antonyms)
 within one language are translated to synonyms (antonyms) within the other.
 Using this insight, we construct monolingual graphs, such as synonym graphs,
 and extend the formulation of [Haghighi]'s 
\emph on
Matching CCA
\emph default
 algorithm to use them.
 We investigate the influence of monolingual graphs and show our new method
 yields superior results in terms of robustness to noise (on synthetic data)
 as well as the precision-recall quality of the output bilexicon (on real
 data).
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
we extend [Haghighi]'s elegant approach for lexicon learning by using monolingua
l graphs, thereby exploiting similarities in both the latent space and the
 original features space.
 We construct supervised and unsupervised monolingual graphs.
\end_layout

\begin_layout Standard
Using a generative story for bilingual word pairs, [Haghighi] shows how
 translations can be obtained by matching according word similarities in
 a learned common latent space.
 
\end_layout

\begin_layout Subsection*
Related work
\end_layout

\begin_layout Standard
[Hal's student] approach avoids learning a common latent space for the two
 word domains.
 They compute a non-linear feature mapping of the words, and subsequently
 obtain a kernel representing the word similarity within each lanauge.
 The matching is then found by kernel sorting [ref].
\end_layout

\begin_layout Standard
Our work can be seen as combining the best of both words, exploiting both
 similarities in the original space, and the latent space -- like [Haghighi],
 we compute similarities in the latent space, however, the usage of monolingual
 graphs encourages learning a matching that respects similarities in the
 original space.
 That is, under our formulation, two words are similar if their latent represent
ation is similar, and the latent representations of their neighbors are
 also similar (see figure ? for an illustration)
\end_layout

\begin_layout Standard
Kernels and Graphs / diffusion kernels?
\end_layout

\begin_layout Section*
Optimization
\end_layout

\begin_layout Standard
(note: the following two subsections can probably be united to a single
 one)
\end_layout

\begin_layout Subsection*
The CCA problem
\end_layout

\begin_layout Standard
Given a list of 
\begin_inset Formula $n$
\end_inset

 pairs of samples 
\begin_inset Formula $\{(x_{i},y_{i})\}_{i=1}^{n}$
\end_inset

 (that is, a matching), let 
\begin_inset Formula $X\in\mathbb{R}^{n\times d_{X}}$
\end_inset

 denote 
\begin_inset Formula $(x_{1},\ldots,x_{n})$
\end_inset

 and similarly let 
\begin_inset Formula $Y\in\mathbb{R}^{n\times d_{Y}}$
\end_inset

 denote 
\begin_inset Formula $(y_{1},\ldots,y_{m})$
\end_inset

.
 The CCA problem for finding the top 
\begin_inset Formula $p$
\end_inset

 correlation coefficients between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 can be formulated as the following optimization program [Hardoon]:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{U,V} & Tr[U^{T}X^{T}YV]\\
s.t. & U^{T}X^{T}XU=I_{p} & V^{T}Y^{T}YV=I_{p}\\
 & u_{i}^{T}X^{T}Yv_{j}=0 & \forall i\ne j\in\{1\ldots p\}
\end{eqnarray*}

\end_inset

Where 
\begin_inset Formula $U\in\mathbb{R}^{d_{X}\times p}$
\end_inset

 and 
\begin_inset Formula $V\in\mathbb{R}^{d_{Y}\times p}$
\end_inset

 serve to project 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 (resp.) to a common latent space.
\end_layout

\begin_layout Standard
Note that the objective function can be expressred 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $Tr[U^{T}X^{T}YV]=Tr[YVU^{T}X^{T}]=\sum_{i=1}^{n}\langle Ux_{i},Vy_{i}\rangle$
\end_inset

.
 Thus we see that CCA's objective is to find projection matrices that maximize
 the overall similarity between projections of the given matched sample
 pairs 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

.
\end_layout

\begin_layout Standard
Although not a convex optimization programming instance, it is well known
 the problem can be solved by singular value decomposition of the matrix
 
\begin_inset Formula $(X^{T}X)^{-\frac{1}{2}}X^{T}Y(Y^{T}Y)^{-\frac{1}{2}}$
\end_inset

 [Hardoon, Section 6].
\end_layout

\begin_layout Subsection*
The Matching CCA problem
\end_layout

\begin_layout Standard
The matching CCA problem [Haghighi] assumes no knowledge of the correct
 matching between the samples in 
\begin_inset Formula $X$
\end_inset

 to those in 
\begin_inset Formula $Y$
\end_inset

.
 Instead, the goal is to recover both the correct matching, as well as find
 a latent space in which the projected matched words are similar.
\end_layout

\begin_layout Standard
Denoting the set of all permutation matrices on 
\begin_inset Formula $n$
\end_inset

 elements as 
\begin_inset Formula $\mathbb{P}_{n}=\{P\in\{0,1\}^{n\times n}|P1=1,\: P^{T}1=1\}$
\end_inset

, we can succinctly write the optimization problem as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{U,V,P} & Tr[U^{T}X^{T}PYV]\\
s.t. & P\in\mathbb{P}_{n}\\
 & U^{T}X^{T}XU=I_{p} & V^{T}Y^{T}YV=I_{p}\\
 & u_{i}^{T}X^{T}PYv_{j}=0 & \forall i\ne j\in\{1\ldots p\}
\end{eqnarray*}

\end_inset

Where the permutation 
\begin_inset Formula $P$
\end_inset

 defines the matching between samples (i.e., rows) in 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
Haghighi suggest an EM-style optimization approach for solving the problem:
 In the M-step the matrices 
\begin_inset Formula $U,V$
\end_inset

 are re-estimated, by considering a fixed
\emph on
 partial 
\emph default
matching derived from 
\begin_inset Formula $P$
\end_inset

 -- A partial matching is required since the matching proposed by the first
 few iterations are expected to be far from the optimal one, and as such
 useless for CCA -- only sample pairs with already high similarity are used
 when estimating the next projection matrices.
\end_layout

\begin_layout Standard
In the E-steps, the projection matrices 
\begin_inset Formula $U,V$
\end_inset

 are held fixed and the problem reduces to the maximum matching problem
 in 
\begin_inset Formula $P$
\end_inset

, which is known to be polynomial-time solvable (for example, by using the
 so-called Hungarian algorithm [ref]).
 
\end_layout

\begin_layout Standard
(technical note:) The non-convexity of the program calls for a good starting
 point.
 [Haghighi] initialize by using a seed matching of small size, obtained
 from the ground truth bilexicon.
 Stability in the latent representation is obtained by keep the seed fixed
 along the optimization procedure.
\end_layout

\begin_layout Subsection*
Intuition Behind Monolingual Graphs
\end_layout

\begin_layout Standard
explain intuition behind monolingual graphs - constructing graphs under
 which we expect neighbors of matches words to be similar.
 The graphs contain no self-loops and so, we don't encode over all pairwise,
 but instead similarity between neighbors.
 Technically may want just a few neighbors to be similar, but, we resort
 to solving over all pairs (expectation).
 There should be a nice figure/graph explaining this -- i.e., we should find
 a pair of synonyms in english and their corresponding in spanish, where
 the correspondance is clear.
\end_layout

\begin_layout Standard
Fixing 
\begin_inset Formula $U,V$
\end_inset

 let 
\begin_inset Formula $\tilde{X}=XU$
\end_inset

 and 
\begin_inset Formula $\tilde{Y}=YV$
\end_inset

 and let 
\begin_inset Formula $G_{X},G_{Y}$
\end_inset

 denote monolingual graphs on the samples of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 (resp.).
\end_layout

\begin_layout Standard
The E-step under our formulation can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{P} & Tr[P\tilde{Y}\tilde{X}^{T}]+\lambda Tr[PG_{Y}\tilde{Y}\tilde{X}^{T}G_{X}^{T}]\\
s.t. & P\in\mathbb{P}_{n}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section*
Experimental Results
\end_layout

\begin_layout Subsection*
Synthetic Data 
\end_layout

\begin_layout Standard
Discuss how the data was generated, and refer to plot that shows our superior
 probability of accurate recovery under increasing noise in both the data
 and graph.
\end_layout

\begin_layout Subsection*
Real Data
\end_layout

\begin_layout Standard
Show results of real experiments
\end_layout

\begin_layout Section*
Discussion
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-21"

\end_inset

David R.
 Hardoon , Sandor Szedmak and John Shawe-Taylor.
 Canonical correlation analysis; An overview with application to learning
 methods http://eprints.soton.ac.uk/259225/1/tech_report03.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-24"

\end_inset

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein, Learning
 Bilingual Lexicons from Monolingual Corpora, http://www.aclweb.org/anthology/P/P0
8/P08-1088.pdf
\end_layout

\end_body
\end_document
