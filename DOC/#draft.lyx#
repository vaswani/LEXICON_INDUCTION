#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 1cm
\bottommargin 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Improved Bilingual Lexicon Learning using Monolingual Graphs
\end_layout

\begin_layout Abstract
In translating between two languages, it is likely that synonyms within
 the source language are translated to synonyms within the target language.
 Motivated by this observation, we construct monolingual graphs, such as
 synonym graphs, and extend [Haghighi]'s matching CCA algorithm to respect
 them.
 We show that using supervised and unsupervised monolingual graphs leads
 to an algorithm that is more robust to noise (shown on synthetic data)
 as well as produces improved precision-recall quality bilexicon (on real
 data).
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Itemize
Translation as a matching problem and [Haghighi]'s mCCA approach
\end_layout

\begin_layout Itemize
Our insight for using monolingual graphs + figure 1 as an illustration -
 There should be a nice figure explaining this -- i.e., we should find a pair
 of synonyms in english and their corresponding in spanish, where the correspond
ance is clear.
 For example, English (President-Chief) and Spanish (Presidente-jefe)
\end_layout

\begin_layout Standard
some rough text:
\end_layout

\begin_layout Standard
We extend [Haghighi]'s elegant approach for lexicon learning by using monolingua
l graphs, thereby exploiting similarities in both the latent space and the
 original features space.
 We construct supervised and unsupervised monolingual graphs.
\end_layout

\begin_layout Standard
Using a generative story for bilingual word pairs, [Haghighi] shows how
 translations can be obtained by matching according to word similarities
 in a learned common latent space.
 
\end_layout

\begin_layout Section
Solving the Matching Problem
\end_layout

\begin_layout Standard
(note: the following two subsections should probably be joined to a single
 that just presents mCCA and says that if P is fixed to the identity along
 the optimization, it just reduces to the ordinary CCA problem.
 This may save some space.)
\end_layout

\begin_layout Subsection
CCA 
\end_layout

\begin_layout Standard
Given a list of 
\begin_inset Formula $n$
\end_inset

 pairs of samples 
\begin_inset Formula $\{(x_{i},y_{i})\}_{i=1}^{n}$
\end_inset

 let 
\begin_inset Formula $X\in\mathbb{R}^{n\times d_{X}}$
\end_inset

 denote 
\begin_inset Formula $(x_{1},\ldots,x_{n})$
\end_inset

 and similarly let 
\begin_inset Formula $Y\in\mathbb{R}^{n\times d_{Y}}$
\end_inset

 denote 
\begin_inset Formula $(y_{1},\ldots,y_{m})$
\end_inset

.
 The CCA problem for finding the top 
\begin_inset Formula $p$
\end_inset

 correlation coefficients between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 can be cast as the following optimization program [Hardoon]:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{U,V} & Tr[U^{T}X^{T}YV]\\
s.t. & U^{T}X^{T}XU=I_{p} & V^{T}Y^{T}YV=I_{p}\\
 & u_{i}^{T}X^{T}Yv_{j}=0 & \forall i\ne j\in\{1\ldots p\}
\end{eqnarray*}

\end_inset

Here, 
\begin_inset Formula $U\in\mathbb{R}^{d_{X}\times p}$
\end_inset

 and 
\begin_inset Formula $V\in\mathbb{R}^{d_{Y}\times p}$
\end_inset

 serve to project the samples in 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 (respectively) to a common latent space of dimension 
\begin_inset Formula $p$
\end_inset

.
 By expressing the objective function 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
as 
\begin_inset Formula $Tr[U^{T}X^{T}YV]=Tr[YVU^{T}X^{T}]=\sum_{i=1}^{n}\langle Ux_{i},Vy_{i}\rangle$
\end_inset

 we see it as a sum of similarities between the matched sample pairs 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

, where the similarity is computed after projection to a common latent space
 defined by 
\begin_inset Formula $U$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

.
 CCA, with its constraints, tries to maximize the total similarity in that
 latent space.
\end_layout

\begin_layout Standard
Although not a convex optimization programming instance, it is well known
 the problem can be solved by singular value decomposition (SVD) of the
 matrix 
\begin_inset Formula $(X^{T}X)^{-\frac{1}{2}}X^{T}Y(Y^{T}Y)^{-\frac{1}{2}}$
\end_inset

 [Hardoon, Section 6].
\end_layout

\begin_layout Subsection
The Matching CCA problem
\end_layout

\begin_layout Standard
Contrary to CCA, The matching CCA problem [Haghighi] assumes no knowledge
 of the correct matching between the samples in 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

, that is we are given 
\begin_inset Formula $\{x_{i}\}_{i=1}^{n}$
\end_inset

 and 
\begin_inset Formula $\{y_{i}\}_{i=1}^{n}$
\end_inset

 and do not know the correspondence between the two sets.
 The goal of matching CCA is to recover the correct matching, as well as
 to find a latent space in which the projected matched words are similar.
\end_layout

\begin_layout Standard
In presenting the problem, we take a non-probabilisitc approach for -- denoting
 the set of all permutation matrices on 
\begin_inset Formula $n$
\end_inset

 elements as 
\begin_inset Formula $\mathbb{P}_{n}=\{\Pi\in\{0,1\}^{n\times n}|\Pi1=1,\:\Pi^{T}1=1\}$
\end_inset

, we can succinctly write the optimization program as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{\Pi,U,V} & Tr[U^{T}X^{T}\Pi YV]\\
s.t. & \Pi\in\mathbb{P}_{n}\\
 & U^{T}X^{T}XU=I_{p} & V^{T}Y^{T}YV=I_{p}\\
 & u_{i}^{T}X^{T}\Pi Yv_{j}=0 & \forall i\ne j\in\{1\ldots p\}
\end{eqnarray*}

\end_inset

Where the permutation matrix 
\begin_inset Formula $\Pi$
\end_inset

 defines the matching between samples (rows) in 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $U,V$
\end_inset

 are as before.
\end_layout

\begin_layout Standard
To solve the problem [Haghighi] suggest an EM-style approach, where the
 hidden variable is 
\begin_inset Formula $\Pi$
\end_inset

: 
\end_layout

\begin_layout Standard
In the E-steps, the projection matrices 
\begin_inset Formula $U,V$
\end_inset

 are held fixed, in which case the problem reduces to the polynomial-time
 solvable maximum [ref Hungarian] weighted-matching problem in 
\begin_inset Formula $\Pi$
\end_inset

, over weight matrix 
\begin_inset Formula $W=YVU^{T}X^{T}\in\mathbb{R}^{n\times n}$
\end_inset

.
 Note that 
\begin_inset Formula $W_{ij}=\langle Ux_{i},Vy_{j}\rangle$
\end_inset

 is the inner product between 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $y_{j}$
\end_inset

 in the latent space.
\end_layout

\begin_layout Standard
In the M-step, the permutation 
\begin_inset Formula $\Pi$
\end_inset

 remains fixed and thus defines a weighted matching between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 (along with the corresponding weights from 
\begin_inset Formula $W$
\end_inset

).
 Then, the matrices 
\begin_inset Formula $U,V$
\end_inset

 are re-estimated by applying CCA only on a partial matching, composed of
 those matched word pairs with weight higher than some threshold 
\begin_inset Foot
status open

\begin_layout Plain Layout
Intuitively, when CCA is applied on a far-from-optimal matching between
 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

, the projection matrices 
\begin_inset Formula $U,V$
\end_inset

 p will be too noisy and the latent space representation would not capture
 anything meaningful.
 This is why a threshold is required.
 Practically, the matching becomes better at each layer, which is why the
 threshold is also gradually decreasing.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The non-convexity of the program calls for a good starting point.
 [Haghighi] initialize by using a seed matching of small size, obtained
 from the ground truth bilexicon.
\end_layout

\begin_layout Subsection
Related Work
\end_layout

\begin_layout Standard
[Jagaralmudi] also view the problem as a matching problem, however, they
 avoid learning a common latent space relating the two word domains.
 Instead, they first compute kernels 
\begin_inset Formula $K_{X},K_{Y}$
\end_inset

 within each set separately, each representing similarities within a word
 domain.
 They proceed to treat the kernels as non-linear feature mappings, such
 that the 
\begin_inset Formula $i$
\end_inset

 row of 
\begin_inset Formula $K_{X}$
\end_inset

 is a new feature representation for the 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $i$
\end_inset

th word in 
\begin_inset Formula $X$
\end_inset

.
 Roughly speaking, the matching is then found by kernelized sorting between
 
\begin_inset Formula $W_{X}=K_{X}^{T}K_{X}$
\end_inset

 and 
\begin_inset Formula $W_{Y}=K_{Y}^{T}K_{Y}$
\end_inset

.
\end_layout

\begin_layout Standard
Viewing 
\begin_inset Formula $K_{X}$
\end_inset

 as weights on a complete graph on the sample in 
\begin_inset Formula $X$
\end_inset

, we see that the weight between 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $x_{j}$
\end_inset

 is exactly 
\begin_inset Formula $[W_{X}]_{ij}=\sum_{m}[K_{X}]_{im}[K_{X}]_{mj}$
\end_inset

 which is the sum of all weighted paths of length 2 from 
\begin_inset Formula $i$
\end_inset

 to 
\begin_inset Formula $j$
\end_inset

, on the complete graph (and by symmetry this view holds also for 
\begin_inset Formula $W_{Y}$
\end_inset

).
\end_layout

\begin_layout Standard
Thus, our work can be seen as combining the best of [Jagaralmudi] and [Haghighi]
, by exploiting similarities in both the original domain and the latent
 space -- Like [Haghighi] we compute similarities in the latent space, however,
 the usage of monolingual graphs, constructed using only domain specific
 knowledge, encourages learning a matching that also respects similarities
 in those domain.
\end_layout

\begin_layout Standard
More broadly, the notion of computing similarities on samples that are known
 to lie on a known graph is neatly addressed by the work of [] on diffusion
 kernels.
\end_layout

\begin_layout Section
Adding Monolingual Graphs to the optimization
\end_layout

\begin_layout Standard
explain intuition behind monolingual graphs - constructing graphs under
 which we expect neighbors of matches words to be similar in the latent
 space.
 The graphs contain no self-loops and so we only encourage similarity between
 neighbors.
 Technically may want just a few neighbors to be similar, but, we resort
 to solving over all pairs (expectation - explain what is GX).
 
\end_layout

\begin_layout Standard
Fixing 
\begin_inset Formula $U,V$
\end_inset

 let 
\begin_inset Formula $\tilde{X}=XU$
\end_inset

 and 
\begin_inset Formula $\tilde{Y}=YV$
\end_inset

 and let 
\begin_inset Formula $G_{X},G_{Y}$
\end_inset

 denote monolingual graphs on the samples of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 (resp.).
\end_layout

\begin_layout Standard
The E-step under our formulation can be written as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\max_{P} & Tr[P\tilde{Y}\tilde{X}^{T}]+\lambda Tr[PG_{Y}\tilde{Y}\tilde{X}^{T}G_{X}^{T}]\\
s.t. & P\in\mathbb{P}_{n}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Subsection
Synthetic Data 
\end_layout

\begin_layout Standard
Discuss how the data was generated, and refer to plot that shows our superior
 probability of accurate recovery under increasing noise in both the data
 and graph.
\end_layout

\begin_layout Subsection
Real Data
\end_layout

\begin_layout Standard
Show results of real experiments with unsupervised graphs, compare resulst
 when using 
\end_layout

\begin_layout Itemize
KNN graphs, 
\end_layout

\begin_layout Itemize
distance weighted KNN graphs
\end_layout

\begin_layout Itemize
eps-graphs
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
What we did.
 Our success.
 Vision.
\end_layout

\begin_layout Itemize
forging good graphs.
\end_layout

\begin_layout Itemize
finding a better generative story that uses graphs, or respects the clustering
 effect of words.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-21"

\end_inset

David R.
 Hardoon , Sandor Szedmak and John Shawe-Taylor.
 Canonical correlation analysis; An overview with application to learning
 methods http://eprints.soton.ac.uk/259225/1/tech_report03.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-24"

\end_inset

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein, Learning
 Bilingual Lexicons from Monolingual Corpora, http://www.aclweb.org/anthology/P/P0
8/P08-1088.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Jagadeesh Jagaralmudi, Seth Juarez, Hal DaumeKernelized Sorting for Natural
 Language Processing 
\end_layout

\end_body
\end_document
